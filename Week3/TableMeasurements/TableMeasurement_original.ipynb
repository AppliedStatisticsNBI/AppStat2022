{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Length Measurements\n",
    "\n",
    "Python program for analysing measurements of the length of the lecture table in Auditorium A at NBI.  \n",
    "There are two measurements each with estimated error of the table length:\n",
    "1. Measurement with a 30cm ruler.\n",
    "2. Measurement with a 2m folding ruler.\n",
    "\n",
    "Each person was asked not only to state the measurement, but also their (gu)estimated uncertainty. None of the persons could see others measurements in order to get the largest degree of independence. Also, the 30cm ruler measurement was asked to be done first. Finally, those measuring were asked to try to measure to the mm, even if precision was less than this, and not to correct any measurement once written down.\n",
    "\n",
    "### Authors: \n",
    "- Troels C. Petersen (Niels Bohr Institute)\n",
    "\n",
    "### Date:    \n",
    "- 24-11-2022 (latest update)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                     # Matlab like syntax for linear algebra and functions\n",
    "import matplotlib.pyplot as plt                        # Plots and figures like you know them from Matlab\n",
    "from iminuit import Minuit                             # The actual fitting tool, better than scipy's\n",
    "import sys\n",
    "from scipy import stats\n",
    "from scipy.special import erfc                         # Error function, to get integral of Gaussian\n",
    "\n",
    "sys.path.append('../../External_Functions')\n",
    "from ExternalFunctions import Chi2Regression, BinnedLH, UnbinnedLH\n",
    "from ExternalFunctions import nice_string_output, add_text_to_ax # useful functions to print fit results on figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for the program: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blinded = True            # Add a random (but fixed) offset to the 30cm and 2m data seperately\n",
    "save_plots = False\n",
    "\n",
    "r = np.random             # Random generator\n",
    "r.seed(42)                # Set a random seed (but a fixed one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data analysis:\n",
    "\n",
    "Before we even look at the look at the data, we decide whether or not we want to blind the analysis by adding a constant to all measurements. This is a good way of working, as we then don't get affected by prior beliefs. Note that a different blinding constant is added for the 30cm ruler and 2m folding rule measurements, so that only after unblinding can the correspondence between the two be considered as a vital cross check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if blinded:\n",
    "    blinding30cm = r.normal(0, 0.1)      # I add a constant (Gaussian with +-10cm) to remain \"blind\"\n",
    "    blinding2m   = r.normal(0, 0.1)      # I add a constant (Gaussian with +-10cm) to remain \"blind\"\n",
    "else:\n",
    "    blinding30cm = 0\n",
    "    blinding2m = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what datafiles we want to look at. Extend it to suit your analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiles = [\"data_TableMeasurements2022.txt\",\n",
    "           \"data_TableMeasurements2021.txt\",\n",
    "           \"data_TableMeasurements2020.txt\",\n",
    "           \"data_TableMeasurements2019.txt\",\n",
    "           \"data_TableMeasurements2018.txt\",\n",
    "           \"data_TableMeasurements2017.txt\",\n",
    "           \"data_TableMeasurements2016.txt\",\n",
    "           \"data_TableMeasurements2015.txt\",\n",
    "           \"data_TableMeasurements2014.txt\",\n",
    "           \"data_TableMeasurements2013.txt\",\n",
    "           \"data_TableMeasurements2012.txt\",\n",
    "           \"data_TableMeasurements2011.txt\",\n",
    "           \"data_TableMeasurements2010.txt\",\n",
    "           \"data_TableMeasurements2009.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in all the data from the `infiles` files (and print the values as a cross check):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L30cm = np.array([])\n",
    "eL30cm = np.array([])\n",
    "L2m = np.array([])\n",
    "eL2m = np.array([])\n",
    "\n",
    "# Loop over files and open them\n",
    "for infile in infiles:\n",
    "    \n",
    "    tmp_L30cm, tmp_eL30cm, tmp_L2m, tmp_eL2m = np.loadtxt(infile, skiprows=2, unpack=True)\n",
    "    \n",
    "    # Note that blinding is applied before storing the values read:\n",
    "    L30cm = np.append(L30cm, tmp_L30cm + blinding30cm)\n",
    "    eL30cm = np.append(eL30cm, tmp_eL30cm)\n",
    "    L2m = np.append(L2m, tmp_L2m + blinding2m)\n",
    "    eL2m = np.append(eL2m, tmp_eL2m)\n",
    "    \n",
    "N_read  = len(L30cm)       # Number of measurements read in total\n",
    "print(L30cm)\n",
    "print(f\"\\n\\nRead all {len(infiles)} file(s) which included {N_read} measurements. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First thing: Plot the data!\n",
    "\n",
    "It is paramount that one plots data - it is the only way to get a good sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully consider binning - this might have to be changed, and subplots/inserts are defintely worth considering!\n",
    "Nbins = 500\n",
    "minL = 0.0\n",
    "maxL = 5.0\n",
    "binwidth = (maxL-minL)/Nbins\n",
    "\n",
    "# Define two histograms with all the lengths recorded:\n",
    "fig_raw, ax = plt.subplots(nrows=2, figsize=(16,14), gridspec_kw={'hspace':0.5})\n",
    "ax_L30cm, ax_L2m = ax\n",
    "\n",
    "hist_L30cm = ax_L30cm.hist(L30cm, bins=Nbins, range=(minL, maxL), histtype='step', label='30cm data')\n",
    "ax_L30cm.set_title('Length estimates by 30cm ruler',fontsize=15)\n",
    "ax_L30cm.set_ylabel('Frequency / 0.01m',fontsize=15)\n",
    "ax_L30cm.set_xlabel('Table length (m)',fontsize=15)\n",
    "\n",
    "hist_L2m = ax_L2m.hist(L2m, bins=Nbins, range=(minL, maxL), histtype='step', label='2m data')\n",
    "ax_L2m.set_title('Length estimates by 2m ruler',fontsize=15)\n",
    "ax_L2m.set_ylabel('Frequency / 0.01m',fontsize=15)\n",
    "ax_L2m.set_xlabel('Table length (m)',fontsize=15)\n",
    "\n",
    "fig_raw.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 cm ruler:\n",
    "\n",
    "We consider the measurements for the 30cm ruler and focus on that for now. Below is a mean and standard deviation (Std) calculation along with a general Gaussian fit to all the data. Somehow, the Std doesn't seem optimal/right...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"  Initial estimate of raw data (30 cm ruler):  {L30cm.mean():.3f} +- {L30cm.std(ddof=1)/np.sqrt(len(L30cm)):.3f} m     (Std = {L30cm.std(ddof=1):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "From the above, you should by now be aware, that there is definitely room for improvement, due to some poor measurement. You want to exclude (or correct?) these, but remember, you should be able to argue for each and every point you remove or correct.\n",
    "\n",
    "In fact, make sure that PRINT OUT EVERY MEASUREMENT THAT YOU EXCLUDE, along with numbers arguing their case! Barlow 4.2.3 (and the note above this very short section) along with Chauvenet's Criterion might serve as guidelines.\n",
    "\n",
    "It is now up to you to select in (based on solid **quantified** arguments) the data, and subsequently produce a better estimate of the length of the table in world famous Auditorium A. Good luck..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data (perhaps good before printing?) and then exclude those you find dubious:\n",
    "L30cm = np.sort(L30cm)\n",
    "mask = (L30cm >= -100.0) * (L30cm <= 100.0)      # Write your own data selection!!!\n",
    "\n",
    "L30cm_subset = L30cm[mask]\n",
    "print(f\"  The number of measurements used before and after is: {len(L30cm):d} and {len(L30cm_subset):d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Start by taking a close look at the data, first by inspecting the numbers in the data file (yeps, open the damn thing, and look over the numbers!), and then by considering the histograms produced by running the notebook. To begin with, only consider the 30cm ruler measurements, and disregard the estimated/guessed uncertainties. You can then expand from there, as guided below by questions.\n",
    "\n",
    "\n",
    "# Questions:\n",
    "\n",
    "1. Consider the mean, standard deviation, and uncertainty on the mean. Is the result as you would expect it? And do you think that it is close to the best possible (i.e. most accurate and precise) estimate? NOTE: Make sure that you know the difference between accuracy and precision!!! See \"Common definition\" in: http://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "\n",
    "2. Do any of the measurements looks wrong/bad/suspicious? Do you see any repeated mistakes done for obvious reasons? Would you correct or exclude any of the measurements and how would you justify this? This problem requires that you discuss with your peers, and then each do what you think most justified/best. It somehow boils down to how far away a measurement is from the expectation.\n",
    "Apply the criterion you find fitting to the list of measurements, and perhaps produce a new list with your accepted and possibly corrected measurements in (to save the original data). How many measurements did you throw away in the end?\n",
    "\n",
    "3. Fit your accepted length measurements with a Gaussian distribution, possibly in a (small?) range around what you believe is the true value. What would be your strategy for choosing a range? Is the Gaussian distribution justified? Also, do you see any \"human\" effects? Did any of your class mates (or you?) not read to mm precision and rounded the result?\n",
    "\n",
    "4. Once you have selected the measurements you want to use, calculate the mean, standard deviation, and uncertainty on the mean. How much did your result improve in precision from the raw data in question 1?\n",
    "\n",
    "5. Now consider also the uncertainties. Try to calculate the weighted mean. Did you get a good Chi2 probability, when doing so? Or are some measurements and/or uncertainties doubtful?\n",
    "\n",
    "6. In order to consider measurements in light of their uncertainties, one uses the \"Pull\" distribution. It is defined as the plot of $z_i = \\left(x_i - \\overline{x} \\right)/\\sigma_i$ where $\\overline{x}$ is the *sample* mean of $x$, and $x_i$ and $\\sigma_i$ are the *single measurements* and their corresponding uncertainties. If the measurements and uncertainties are good, then it should give a unit Gaussian. Is that the case? And thus, were the uncertainty estimates/guesses reasonable? If not, then the pull distribution is often used to remove overly precise measurements (those far out on the tails), and afterwards to scale the errors on the remaining measurements to a reasonable level. Consider the pull distribution, and see if any measurements have suspicious uncertainties. Did the Chi2 probablity improve? And did the result improve further in precision? If not, why could that be?\n",
    "\n",
    "7. Is the number of mismeasurements 30cm below the actual length more common than those 30cm above? Determine the ratio $r_{mis} = N_{30cm~high} / N_{30cm~low}$ including both its statistical uncertainty, and if possible a systematic uncertainty from defining exactly when a measurement is 30cm off.\n",
    "\n",
    "\n",
    "\n",
    "#### Now repeat the above for the 2m folding rule\n",
    "...and consider all the questions below as optional problems, where you might just try to solve selected problems.\n",
    " \n",
    "***\n",
    "\n",
    "8. How much better/worse is the single measurement uncertainty from the 30cm ruler case to the 2m folding rule?\n",
    "\n",
    "9. Does the length of the table seems to be different when measured with a 30cm and a 2m ruler? Quantify this statement! I.e. what is the difference, and what is the uncertainty on that difference? Is this significantly away from 0?\n",
    "\n",
    "10. If you were asked for the best estimate of the length of the table, what would you do? (If posssible, read Bevington page 58 bottom!)\n",
    "\n",
    "\n",
    "### Advanced questions:\n",
    "\n",
    "11. Is there any correlation between the errors on the measurements and the distance value? I.e. do you see any effect of those measuring e.g. too long having a smaller/larger uncertainty? What would the effect of this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "This is a \"free\" and potentially long exercise, which in essense is meant to illustrate how challeging real data can be, even if it is conceptually simple and in one dimension.\n",
    "\n",
    "The first learning point is that real data is potentially very complicated, and requires consideration/processing before being as clear cut, as what we simulate. Two approaches could be:\n",
    "1. A crude/fast *\"cut the non-standard (wrong?) measurements away\"* analysis, potentially missing something.\n",
    "2. A detailed *\"plot a lot and consider every single point\"*, potentially (over-?)using a lot of time.\n",
    "\n",
    "From this exercise, you should have learned:\n",
    "1. Always **start by plotting the data**, and ask yourself, if the distributions make sense.\n",
    "2. In order to remove potential mis-measurements, one uses **Chauvenet's Criterion** and rejects points with a low probability of being correct.\n",
    "3. Alternatively, one corrects data points (calibration) based on **understanding the reason(s) for these shifts**. \n",
    "4. Once the data has been cleaned (distributions Gaussian?), one can apply the usual calculations of mean $\\mu$, Standard Deviation $\\sigma$, and uncertainty on the mean $\\sigma_{\\mu}$.\n",
    "5. When considering **data with uncertainties, the data cleaning is different**, as one considers how far away potential mis-measurements are in units of their uncertainty.\n",
    "6. **It is always important to have ways of cross checking one's results**, in this case comparing the 30cm ruler with the 2m folding rule result."
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
