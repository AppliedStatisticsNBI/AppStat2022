{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting is an Art!\n",
    "\n",
    "### Description\n",
    "Python macro for testing which fitting procedure is likely to give the \"best\" results. The three cases in question are:\n",
    "   * Linear function with x-values far from zero.\n",
    "   * Gaussian distribution(s?) on constant background (peak searching)\n",
    "   * Double exponential distribution (high correlations)\n",
    "\n",
    "### Your Task\n",
    "Consider each case and argue/discuss which fitting function and method should be used.\n",
    "\n",
    "***\n",
    "### Authors:\n",
    "- Troels Petersen ([email](mailto:petersen@nbi.dk))\n",
    "\n",
    "### Last update:\n",
    "- 6th of January 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from iminuit import Minuit\n",
    "from scipy import stats\n",
    "import os, sys                                         # Modules to see files and folders in directories\n",
    "from os.path import dirname as parent_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../External_Functions')\n",
    "from ExternalFunctions import Chi2Regression, BinnedLH, UnbinnedLH\n",
    "from ExternalFunctions import nice_string_output, add_text_to_ax    # Useful functions to print fit results on figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set som plotting standards:\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavePlots = False         # Determining if plots are saved or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "r = np.random                         # Random generator\n",
    "r.seed(42)                            # Set a random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CASE 1: Linear fit with x-values far from zero:\n",
    "\n",
    "The initial fitting function is the following:\n",
    "\n",
    "* $f_{1}(x) = ax + b$\n",
    "\n",
    "If the x-values are far from zero (e.g. the year we are in), then any change in $a$ gives a major change in $b$, and the two gets very correlated. However, if the mean $x$-value (or some value close to it) is subtracted from $a$ in the function, then this is avoided, as $b$ is now the $y$ value at the middle of this point, which is not subject to much change.\n",
    "\n",
    "* $f_{1}(x) = a(x - \\bar{x}) + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =     0.501\n",
    "b = -1000.0\n",
    "sigmay = 0.5\n",
    "\n",
    "Npoints_lin = 11\n",
    "minx = 2010\n",
    "maxx = 2020\n",
    "\n",
    "x = np.arange(minx,maxx+1)\n",
    "y = a*x + b + r.normal(0.0, sigmay, len(x))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_bad(x, a, b) :\n",
    "    return a*x + b\n",
    "\n",
    "def func_good(x, a, b) :\n",
    "    return a*(x-2015) + b\n",
    "\n",
    "fit_chi2 = Chi2Regression(func_bad, x, y, sigmay)\n",
    "minuit_chi2 = Minuit(fit_chi2, a=0.0, b=0.0)\n",
    "minuit_chi2.errordef = 1.0\n",
    "minuit_chi2.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fit = np.linspace(minx, maxx, 1000)\n",
    "y_fit = func_good(x_fit, *minuit_chi2.values[:])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "plot_data = ax.errorbar(x, y, sigmay, fmt='.', linewidth=2, color='blue', label=\"Data\")\n",
    "ax.plot(x_fit, y_fit, '-', color='red', linewidth=2, label='ChiSquare fit')\n",
    "ax.set(xlabel='x (year)', ylabel='y')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CASE 2: Gaussian distribution on a constant background:\n",
    "\n",
    "The initial fitting function is the following:\n",
    "\n",
    "* $f_{1}(x) = C+ \\frac{N}{\\sigma\\sqrt(2\\pi)}\\cdot \\exp \\left[-0.5 \\cdot\\left(\\frac{(x-\\mu)}{\\sigma}\\right)^{2} \\right]$ for $x$ in $[-\\infty,\\infty]$\n",
    "\n",
    "It disregards that there might be additional signal peaks at higher values. Your job is to expand the fit until it really describes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npoints_gauss = 600\n",
    "mux    = 3.50\n",
    "sigmax = 0.25\n",
    "f_core = 0.7\n",
    "\n",
    "Npoints_pol0 = 2000\n",
    "minx   =  0.0\n",
    "maxx   = 10.0\n",
    "Nbins = 100\n",
    "binwidth_gauss = (maxx-minx) / float(Nbins)\n",
    "print(f\"  The bin width is: {binwidth_gauss:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill histogram with signal and background events:\n",
    "signal1_core = np.random.normal(loc=mux,scale=sigmax,    size=int(Npoints_gauss * f_core))\n",
    "signal1_tail = np.random.normal(loc=mux,scale=sigmax*2.5,size=int(Npoints_gauss * (1-f_core)))\n",
    "signal2_core = np.random.normal(loc=mux+4.1,scale=sigmax,    size=int(Npoints_gauss/3.14 * f_core))\n",
    "signal2_tail = np.random.normal(loc=mux+4.1,scale=sigmax*2.5,size=int(Npoints_gauss/3.14 * (1-f_core)))\n",
    "bkg = np.random.uniform(low=minx,high=maxx,size=Npoints_pol0)\n",
    "\n",
    "Y = np.concatenate([signal1_core, signal1_tail, signal2_core, signal2_tail, bkg])\n",
    "binning = np.linspace(minx,maxx,Nbins)\n",
    "counts,bin_edges = np.histogram(Y,bins=binning)\n",
    "unc_count = np.sqrt(counts)\n",
    "X = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.errorbar(X, counts, yerr=unc_count, marker = '.', drawstyle = 'steps-mid')\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('Frequency / 0.1', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function (including bin width to get normalisation right):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_gpol0(x, N, mu, sigma, cst) :\n",
    "    norm = binwidth_gauss * N / np.sqrt(2.0*pi) / sigma\n",
    "    z = (x-mu)/sigma\n",
    "    return norm * np.exp(-0.5*z*z) + cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = counts>0\n",
    "chi2reg = Chi2Regression(func_gpol0, X[select], counts[select], unc_count[select])\n",
    "minuit_obj = Minuit(chi2reg, N=1.0, mu=1.0, sigma=1.0, cst=1.0)\n",
    "minuit_obj.errordef = 1.0\n",
    "minuit_obj.migrad()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not minuit_obj.fmin.is_valid) :                                   # Check if the fit converged\n",
    "    print(\"  WARNING: The ChiSquare fit DID NOT converge!!!\")\n",
    "\n",
    "bf_N, bf_mu, bf_sigma, bf_cst = minuit_obj.values[:]\n",
    "ax.plot(X, func_gpol0(X, bf_N, bf_mu, bf_sigma, bf_cst), 'r', linewidth=2.0, label='Const + Gauss fit')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Look at the first case, but make sure that you also do the second case (see below), as this is\n",
    "at least as important as the first one!\n",
    "\n",
    "Questions on CASE 2:\n",
    "--------------------\n",
    " 1. Look at the data plot and the corresponding fit. What type of fit is it? Does it\n",
    "    run well (or at all)? And once it runs, does it seem to be reasonable? Why/why not?\n",
    "    And does the fitting function include all features of the data? Why/why not? Try for\n",
    "    2-5 minutes and discuss it with others (if possible), before reading on!\n",
    "\n",
    "---\n",
    "_2-5 minutes later_...\n",
    "\n",
    "---\n",
    "\n",
    " 2. What is the p-value from the minimal Chi2 that your fit obtained, once you got any\n",
    "    \"reasonable\" fit to work? Is it acceptable?\n",
    "\n",
    " 3. As it happens, there are tails on the main peak, and there also seem to be a\n",
    "    suspectable bump around 7 < x < 8. Try to write\n",
    "    an expanded fitting function, which includes these features in the model, and get the\n",
    "    fit to run. How significant is the second Gaussian peak, based on significance of the\n",
    "    amplitude? And what test would you apply to this, if you wanted to make a full-fledged\n",
    "    hypothesis test between the two models? Are they nested? Can you actually get a number out?\n",
    "\n",
    "---\n",
    "_10-20 minutes later_...\n",
    "\n",
    "---\n",
    "\n",
    " 4. Imagine that you concluded that there was a new peak, and that you were sure that\n",
    "    it had the same width as the original peak (for example because the width was due to\n",
    "    the resolution of the apperatus, and not the peak itself). Does that help you in the fit,\n",
    "    and if so, how? Does the significance of the peak increase? Would it always do that?\n",
    "    Also imagine, that the parameter of interest is the distance between the peaks. How\n",
    "    would you now write your fitting function?\n",
    "    \n",
    "\n",
    "## NOTE: \n",
    "\n",
    "If one wanted to test the G+pol0 vs. the G+G+pol0 models against each other, which might be relevant, then considering the difference in ChiSquare values or -2ln of the likelihood ratio would be obvious (these two gives the same result, when errors are Gaussian and the binning does not have any effect). Wilk's theorem would provide the way to produce a p-value, thus doing a proper hypothesis test using the likelihood ratio test:\n",
    "\n",
    "* Using iminuit, fit the data with both hypothesis, and note the Chi2 or LLH value (usi `minuit_obj.fval`).\n",
    "* Then compute the test statistic $\\chi^2_{1} - \\chi^2_{2}$ or $-2\\log{\\frac{LH_{1}}{LH_{2}}}$, and see that it is $\\chi^{2}$ distributed (Wilk's Theorem).\n",
    "\n",
    "The test statistic distribution will have $N_{dof} = 3$, unless you (smartly) eliminated one parameter from the second fit. The case when fitting with a double Gaussian for each peak is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## CASE 3: Double exponential distribution\n",
    "\n",
    "Here we are considering the fitting of exponential data, and how the writing of the fitting function is important.\n",
    "\n",
    "* The \"bad\" fitting function:\n",
    "    $f_{2,bad}(t)  = N_{1}\\cdot\\exp(-t/r_{1}) + N_{2}\\cdot\\exp(-t/r_{2})$ for $t$ in $[0,\\infty]$\n",
    "\n",
    "* The \"good\" fitting function:\n",
    "    $f_{2,good}(t) = N \\cdot\\left(\\frac{f}{r_{1}}\\cdot \\exp\\left[-t/r_{1}\\right] + \\frac{(1-f)}{r_{2}}\\cdot\\exp\\left[-t/r_{2}\\right]\\right)$ for $t$ in $[0,\\infty]$\n",
    "\n",
    "## NOTE\n",
    "The parameters $r_1$ and $r_2$ need to be positive, and $f$ in [0,1], in order for this to be a PDF.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npoints_2exp = 2000\n",
    "frac = 0.5   # Fraction that \"belongs to\" first exponential\n",
    "r1 = 10.0\n",
    "r2 =  2.0     # Note to what the two lifetimes are different, as this decides their correlation!\n",
    "nbins_2exp = 200\n",
    "xmin_2exp = 0.0\n",
    "xmax_2exp = 20.0\n",
    "binning = np.linspace(xmin_2exp, xmax_2exp, nbins_2exp)\n",
    "binwidth_2exp = (xmax_2exp - xmin_2exp) / nbins_2exp\n",
    "print(binwidth_2exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pts = np.random.uniform(size=Npoints_2exp) \n",
    "N1 = sum(all_pts<frac)\n",
    "\n",
    "t = np.concatenate([np.random.exponential(scale=r1, size=N1),\n",
    "                    np.random.exponential(scale=r2, size=(Npoints_2exp-N1))])\n",
    "\n",
    "counts,bin_edges = np.histogram(t, bins=binning)\n",
    "unc_2exp = np.sqrt(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.errorbar(X, counts, yerr=unc_2exp, marker = '.', drawstyle = 'steps-mid')\n",
    "ax.set_xlabel('t',fontsize=13)\n",
    "ax.set_ylabel('count',fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to fit the data with the bad model:\n",
    "I include the binwidth (here 0.1) in the fit to ensure that the normalisations are (or could be) right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2expBad(x,N1,r1,N2,r2):\n",
    "    binwidth = binwidth_2exp\n",
    "    return binwidth*(N1*np.exp(-x/r1)+N2*np.exp(-x/r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = counts>0\n",
    "chi2reg = Chi2Regression(f_2expBad, X[select], counts[select], unc_2exp[select])\n",
    "\n",
    "# Random starting values:\n",
    "xf = r.uniform(0.9, 1.1, 4)\n",
    "\n",
    "# The bad fit (notice that we start it at \"perfect\" parameters!!!):\n",
    "minuit_obj = Minuit(chi2reg, N1=xf[0]*frac*Npoints_2exp, r1=xf[1]*r1, N2=xf[2]*(1-frac)*Npoints_2exp, r2=xf[3]*r2)\n",
    "minuit_obj.errordef = 1.0\n",
    "minuit_obj.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the covariance/correlation matrix:\n",
    "\n",
    "As stated, you want your fit to have the **least** correlations between the fitting parameters.<br>\n",
    "Try to check all the entries above, and see if you understand why the correlations are as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not minuit_obj.fmin.is_valid) :                                   # Check if the fit converged\n",
    "    print(\"  WARNING: The ChiSquare fit DID NOT converge!!!\")\n",
    "    \n",
    "bf_N1, bf_r1, bf_N2, bf_r2 = minuit_obj.values[:]\n",
    "Chi2 = minuit_obj.fval\n",
    "Ndof = len(X[select]) - len(minuit_obj.values[:])\n",
    "ProbChi2 = stats.chi2.sf(Chi2, Ndof)\n",
    "print(Chi2, Ndof, ProbChi2)\n",
    "\n",
    "ax.plot(X, f_2expBad(X,bf_N1,bf_r1,bf_N2,bf_r2), 'r', linewidth=2.0, label='Bad fitting function')\n",
    "\n",
    "d = {'Entries'   : \"{:d}\".format(sum(counts)),\n",
    "     'Chi2/d.o.f': \"{:.3f} / {:d}\".format(Chi2, Ndof),\n",
    "     'Prob'      : \"{:.3f}\".format(ProbChi2),\n",
    "     'N1'        : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['N1'], minuit_obj.errors['N1']),\n",
    "     'N2'        : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['N2'], minuit_obj.errors['N2']),\n",
    "     'r1'        : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['r1'], minuit_obj.errors['r1']),\n",
    "     'r2'        : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['r2'], minuit_obj.errors['r2'])}\n",
    "\n",
    "ax.text(0.65, 0.95, nice_string_output(d, 0), family='monospace', \n",
    "        transform=ax.transAxes, fontsize=13, color='red', verticalalignment='top')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions on CASE 3:\n",
    "--------------------\n",
    " 1. Does the \"bad\" fit work well? Does it extract the parameters used to produce it?\n",
    "    Can you see what is missing? There are in fact several things, but one is simple to remedy.\n",
    "    Think and discuss...\n",
    "   \n",
    "---\n",
    "_5-10 minutes later_...\n",
    "\n",
    "---\n",
    "Of course you need to give the fit good initial values! Do this (for example those the data was produced with!), and run it again. It might work now, but actually that is not always the case. The reason is that the \"bad\" fitting function has two flaws:\n",
    "\n",
    "* It does not have a correct normalisation, thus making N1 and N2 correlated, as well as r1 and r2.\n",
    "* It does not have one overall normalisation, thus making N1 and N2 even more correlated.\n",
    "\n",
    "This gives very high correlations between the parameters, as can be seen from the correlation matrix printed.\n",
    "\n",
    " 2. Both of these problems can be mitigated by rewriting the fitting function to include\n",
    "    the correct normalisation (i.e. dividing by the lifetime) and by putting only one\n",
    "    overall normalisation and then dividing the two lifetimes with a fraction (i.e. use\n",
    "    \"frac\" and \"(1.0-frac)\" as a parameter in front of each exponential term).\n",
    "    Try this (define a \"good\" function), and see if your fit improves. The way to see\n",
    "    this would in general be to try a lot of different data, but here we will simply see\n",
    "    that the correlations are smaller (especially for the overall normalisation).\n",
    "---\n",
    "_10-20 minutes later_...\n",
    "\n",
    "---  \n",
    "\n",
    "__If you didn't manage to get this fit going, I've included a \"good\" fitting function below! (but try yourself first!)__\n",
    "\n",
    " 3. The two lifetimes are naturally very correlated with each other (and the fraction),\n",
    "    when they are very alike. The only thing one can do about this is to fix one parameter.\n",
    "    This is of course not desirable, but one can be forced to do it, if the fit does not\n",
    "    converge otherwise. Note that since the correlation is very high, it is not a great\n",
    "    loss of freedom in the fitting function. The correlation between r1 and r2 depends a lot\n",
    "    on how similar they are.\n",
    "    \n",
    "    A very common similar example is fitting a \"Gaussian-like\" peak, which happens to have\n",
    "    more than one width, for example if the data is obtained from two or more sources with\n",
    "    different resolutions (as above). Here, one may choose to let the two (or more) Gaussians have\n",
    "    the same mean, but two different widths (the \"good\" and the \"bad\" measurements).\n",
    "    Typically, the parameter to fix (if any) is the fraction, but never fix a parameter\n",
    "    without first having tried to let it \"float\".\n",
    "    \n",
    "    However, **no fit function is perfect** and the below fitting function is not an \"absolute\"\n",
    "    improvement. The improvement lies only in some parts of the parameter space, and it is hard\n",
    "    to know, when it has been improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GOOD fitting function and fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_exp(x, N, f, r1, r2):\n",
    "    binwidth = binwidth_2exp\n",
    "\n",
    "    # Either \"just\" normalise correctly:\n",
    "    # return binwidth*(N1/r1*np.exp(-x/r1)+N2/r2*np.exp(-x/r2))\n",
    "    # Or also re-write to include f = fraction:\n",
    "    return binwidth*N*(f/r1*np.exp(-x/r1) + (1.0-f)/r2*np.exp(-x/r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = counts>0\n",
    "chi2reg = Chi2Regression(good_exp, X[select], counts[select], unc_2exp[select])\n",
    "\n",
    "minuit_obj = Minuit(chi2reg, N=xf[0]*Npoints_2exp, f=xf[3]*frac, r1=xf[1]*r1, r2=xf[2]*r2)\n",
    "minuit_obj.errordef = 1.0\n",
    "minuit_obj.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not minuit_obj.fmin.is_valid) :                                   # Check if the fit converged\n",
    "    print(\"  WARNING: The ChiSquare fit DID NOT converge!!!\")\n",
    "\n",
    "bf_N, bf_f, bf_r1, bf_r2 = minuit_obj.values[:]\n",
    "Chi2 = minuit_obj.fval\n",
    "Ndof = len(X[select]) - len(minuit_obj.values[:])\n",
    "ProbChi2 = stats.chi2.sf(Chi2, Ndof)\n",
    "print(Chi2, Ndof, ProbChi2)\n",
    "\n",
    "\n",
    "ax.plot(X, good_exp(X, bf_N, bf_f, bf_r1, bf_r2), 'g', linewidth=2.0, label='Good-fit')\n",
    "\n",
    "d = {'Entries'   : \"{:d}\".format(sum(counts)),\n",
    "     'Chi2/d.o.f': \"{:.3f} / {:d}\".format(Chi2, Ndof),\n",
    "     'Prob'      : \"{:.3f}\".format(ProbChi2),\n",
    "     'N'         : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['N'], minuit_obj.errors['N']),\n",
    "     'f'         : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['f'], minuit_obj.errors['f']),\n",
    "     'r1'        : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['r1'], minuit_obj.errors['r1']),\n",
    "     'r2'        : \"{:.3f} +/- {:.3f}\".format(minuit_obj.values['r2'], minuit_obj.errors['r2'])}\n",
    "\n",
    "ax.text(0.3, 0.95, nice_string_output(d, 0), family='monospace', \n",
    "        transform=ax.transAxes, fontsize=13, color='green', verticalalignment='top')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "This exercise in \"fitting tricks\" should teach you that:\n",
    "1. __Good starting values is paramount!__ (Almost all fits fail with poor starting values).\n",
    "2. The form of the fitting function is also important.<br>\n",
    "   a. Ensure that the x-values do not represent some small range far from 0.<br>\n",
    "   b. Ensure that you give the fitting function enough freedom to fit the data.\n",
    "   c. Conversely, try to curb the number of parameters, if there are arguments for doing so (calibration peaks).\n",
    "   d. Make sure that you've normalised your fitting PDFs, to avoid correlations between normalisation and parameters.\n",
    "3. If a fit continues to fail, try simply to draw the function and starting values on top of the data. Often, they don't match well (general advice, not in this exercise)."
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
